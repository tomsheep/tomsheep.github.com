---
layout: post
category : 儒林外史
title: 祛魅（6）
date: 2012-12-16 23:04:42 +08:00
tagline:
tags: [ML, 统计学习, 感知机]
---

### Notes on《统计学习方法》(李航)-2：

1. 感知机（perceptron）是二类线性分类模型：`f(x)=sign(w·x + b)` ，输出为+1或-1；感知机对应于输入空间（特征空间）中的分离超平面`w·x + b = 0` w是超平面的法向量，b是截距。

2. 补一段数学基础：

    * 仿射空间: 给定的n维空间中，k维仿射空间由空间中的一点P和k个线性无关的向量v1,v2,v3,...,vk决定, 空间中的一条直线是一个1维仿射空间，一个平面是一个2维仿射空间。

    * 超平面：在给定的N维空间中，超平面由空间中的一点P和一个向量n决定，超平面方程为 `n*(i-P)=0`，其中i表示超平面上的任意一点。i,n,p均为N维向量。n为超平面的法向量。若`i=(i1,i2,..,iN),n=(n1,n2,..,nN), p=(p1,p2,..pN)`,则超平面方程可以表示成： `n1*i1+n2*i2+...nN*iN+d=0`。二维空间的超平面是一条直线，三维空间的超平面是一个平面，而N维空间的超平面则是N-1维的仿射空间。

    * 点到面的距离：`d=|向量AB*向量n|/向量n的模长` d表示点A到面的距离，向量AB是以点A为起点，以平面上任意一点为终点的向量，向量n是平面的法向量。

3. 感知机的损失函数一般选为误分类点（就是分类错误，`wx_i +b`和`y_i`符号相反的点）到超平面S的总距离（因为这个函数连续可导，而且符合损失函数的要求）；所以感知机学习策略是极小化损失函数：`L(w,b) = -\sum{y_i(wx_i + b)}`, `x_i`为误分类点。

4. 感知机学习算法是基于“随机梯度下降法”对损失函数的最优化算法：随机选取一个超平面，然后一次选一个误分类点使其梯度下降，不断极小化目标函数。这个算法是收敛的，在训练数据集上的误分类次数满足不等式`k&lt;=(R/r)^2`, `r = min(y_i(wx_i + b))`, `R=max||x_i||`

5. 感知机只能处理“线性可分问题”：即特征空间中，存在某个超平面S：`wx+b=0`，使得数据集中的正实例点和负实例点完全正确地划分到超平面两侧。


